# ğŸï¸ Azure F1 Pipeline â€“ FP3 Monaco 

Pipeline de ingenierÃ­a de datos inspirado en arquitectura **Azure Data Lakehouse**, utilizando datos abiertos de **OpenF1**.

El proyecto replica un flujo **RAW â†’ SILVER**, empleando **PySpark** para limpiar, transformar y estandarizar la informaciÃ³n de la **FP3 del GP de MÃ³naco 2023**.  
El siguiente paso (en desarrollo) serÃ¡ la capa **GOLD**, desplegada en **Azure Databricks**.

---

## ğŸ“‚ Estructura del proyecto

```bash
azure-f1-pipeline/
â”‚
â”œâ”€â”€ data/                             # Datos RAW extraÃ­dos de la API de OpenF1
â”‚   â”œâ”€â”€ sessions_2023.csv              # InformaciÃ³n de todas las sesiones de 2023
â”‚   â”œâ”€â”€ drivers_9089.csv               # Datos de pilotos (FP3 MÃ³naco 2023)
â”‚   â””â”€â”€ session_result_9089.csv        # Resultados FP3 MÃ³naco 2023
â”‚
â”œâ”€â”€ scripts/                          # LÃ³gica principal del pipeline
â”‚   â”œâ”€â”€ Extract_csv.py          # Script de extracciÃ³n desde OpenF1 API
â”‚   â””â”€â”€ silver_job.py                  # Transformaciones PySpark (RAW â†’ SILVER)
â”‚
â”œâ”€â”€ silver/                           # Capa Silver (datos limpios listos para anÃ¡lisis)
â”‚   â””â”€â”€ f1_results_silver_fp3_2023.csv # Dataset consolidado y normalizado
â”‚
â”œâ”€â”€ gold/                             # (PrÃ³ximamente) Capa Gold - KPIs y mÃ©tricas agregadas
â”‚
â””â”€â”€ README.md                         # DocumentaciÃ³n principal del proyecto
```

---

## âš™ï¸ Flujo de datos

**1ï¸âƒ£ ExtracciÃ³n (RAW):**  
Mediante `Extract_csv_pretty.py`, se consultan los endpoints CSV pÃºblicos de OpenF1 y se guardan localmente en la carpeta `data/`.

**2ï¸âƒ£ TransformaciÃ³n (SILVER):**  
El job `silver_job.py` utiliza **PySpark** para:
- Cargar los datasets `sessions`, `drivers` y `session_result`.  
- Convertir tipos de datos (`int`, `double`, `timestamp`, `boolean`).  
- Estandarizar mÃ©tricas (`duration_sec`, `laps_completed`).  
- Unir las tablas y derivar el campo `status` (`DSQ`, `DNF`, `DNS`).  
- Generar un CSV final limpio y unificado en `silver/`.

**3ï¸âƒ£ (PrÃ³ximamente) Capa GOLD:**  
Resumen analÃ­tico para Power BI / Synapse:  
- Medias de tiempo por equipo.  
- Gaps promedio respecto al lÃ­der.  
- Ranking de pilotos por sesiÃ³n.

---

## ğŸ§© Stack tecnolÃ³gico

| Componente | TecnologÃ­a | DescripciÃ³n |
|-------------|-------------|-------------|
| Ingesta de datos | **OpenF1 API (CSV endpoints)** | Datos pÃºblicos de sesiones F1 |
| Procesamiento | **PySpark (local / Databricks-ready)** | Limpieza, joins y transformaciones |
| Almacenamiento | **Data Lake (local â†’ Azure Data Lake)** | Estructura RAW / SILVER / GOLD |
| OrquestaciÃ³n *(futuro)* | **Azure Data Factory / Airflow** | AutomatizaciÃ³n de jobs |
| AnÃ¡lisis *(futuro)* | **Power BI / Azure Synapse** | VisualizaciÃ³n de mÃ©tricas F1 |

---

## ğŸš€ EjecuciÃ³n local

### 1. Crear entorno y dependencias
```bash
python3 -m venv venv
source venv/bin/activate
pip install pyspark pandas requests certifi
```

###  2. Descargar los CSVs desde OpenF1

El script `Extract_csv.py` permite descargar los datasets directamente desde los endpoints pÃºblicos de **OpenF1**.

```bash
python3 scripts/Extract_csv.py
```
Esto genera los archivos CSV en la carpeta `data/`:

```bash
data/
 â”œâ”€â”€ sessions_2023.csv
 â”œâ”€â”€ drivers_9089.csv
 â””â”€â”€ session_result_9089.csv
```

### 3. Ejecutar el job de limpieza (RAW â†’ SILVER)
El script `silver_job.py` utiliza PySpark para limpiar, transformar y unir los datasets.

```bash
python3 scripts/silver_job.py
```

El resultado es un dataset limpio y consolidado listo para anÃ¡lisis:

```bash
silver/
 â””â”€â”€ f1_results_silver_fp3_2023.csv
```
---

## ğŸ“Š PrÃ³ximos pasos
- AÃ±adir capa GOLD con agregaciones y KPIs
- Migrar el pipeline a Azure Databricks
- Publicar dashboard en Power BI Service
- AÃ±adir orquestaciÃ³n con Azure Data Factory o Airflow
- Implementar almacenamiento en Azure Data Lake (ADLS Gen2)